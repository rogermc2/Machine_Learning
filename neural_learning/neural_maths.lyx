#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 15mm
\topmargin 10mm
\rightmargin 10mm
\bottommargin 15mm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Equations
\end_layout

\begin_layout Standard
Neural network equations based on 
\begin_inset CommandInset citation
LatexCommand cite
key "unn"
literal "false"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "bsnn"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Forward Propagation
\end_layout

\begin_layout Standard
The output of a neuron is the activation function of a weighted sum of the
 neuron’s input:
\begin_inset Formula 
\begin{eqnarray*}
j_{j} & = & w_{j,1}\times x_{1}+\cdots+w_{j,N}\times x_{N}=\sum_{i=1}^{N}w_{j,i}\times x_{i}+b_{j}
\end{eqnarray*}

\end_inset

 
\end_layout

\begin_layout Standard
where:
\end_layout

\begin_layout Standard
\begin_inset Formula $x_{i}$
\end_inset

 represents the 
\begin_inset Formula $i$
\end_inset

th neuron of layer 
\begin_inset Formula $X$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $y_{j}$
\end_inset

 represents the 
\begin_inset Formula $j$
\end_inset

th neuron of layer 
\begin_inset Formula $Y$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $w_{j,i}$
\end_inset

 represents the coefficient (weight) of a linear equation that maps the
 neurons of layer 
\begin_inset Formula $X$
\end_inset

 to the 
\begin_inset Formula $j$
\end_inset

th neuron of layer 
\begin_inset Formula $Y$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $b_{j}$
\end_inset

 represents the intercept (bias) of a linear equation that maps the neurons
 of layer 
\begin_inset Formula $X$
\end_inset

 to the 
\begin_inset Formula $j$
\end_inset

th neuron of layer 
\begin_inset Formula $Y$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\begin{bmatrix}y_{1}\\
\vdots\\
y_{m}
\end{bmatrix} & = & \begin{bmatrix}w_{1,1} & \cdots & w_{1,n}\\
\vdots & \cdots & \vdots\\
w_{m,1} &  & w_{m,n}
\end{bmatrix}\begin{bmatrix}x_{1}\\
\vdots\\
x_{n}
\end{bmatrix}+\begin{bmatrix}b_{1}\\
\vdots\\
b_{n}
\end{bmatrix}\\
Y & = & W\cdot X+B\\
O_{_{l}} & = & Activation\left(W_{l-1}\cdot I_{l-1}+B_{l-1}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where:
\end_layout

\begin_layout Standard
\begin_inset Formula $I_{l-1}$
\end_inset

 represents input layer 
\begin_inset Formula $l-1$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $O_{l}$
\end_inset

 represents output layer 
\begin_inset Formula $l$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $Activation$
\end_inset

 represents a mathematical function.
\end_layout

\begin_layout Subsection
Back Propagation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
EX & = & \begin{bmatrix}\frac{w_{1,1}}{\sum_{k=1}^{n}w_{1,k}} & \cdots & \frac{w_{n,1}}{\sum_{k=1}^{n}w_{n,k}}\\
\vdots & \cdots & \vdots\\
\frac{w_{1,m}}{\sum_{k=1}^{n}w_{1,k}} & \cdots & \frac{w_{n,m}}{\sum_{k=1}^{n}w_{1,k}}
\end{bmatrix}EY
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where:
\end_layout

\begin_layout Standard
\begin_inset Formula $EX$
\end_inset

 represents the error in X caused by the error 
\begin_inset Formula $EY$
\end_inset

in Y.
\end_layout

\begin_layout Standard
The denominator of the weight ratio, acts as a normalizing factor, so we
 don’t care that much about it, partially because the final equation we
 will have other means of regulating the learning of neural network, so
 it can be ignored:
\begin_inset Formula 
\begin{eqnarray*}
EX & = & \begin{bmatrix}w_{1,1} & \cdots & w_{n,1}\\
\vdots & \cdots & \vdots\\
w_{1,m} & \cdots & w_{n,m}
\end{bmatrix}EY\\
EX & = & \begin{bmatrix}w_{1,1} & \cdots & w_{1,m}\\
\vdots & \cdots & \vdots\\
w_{n,1} & \cdots & w_{n,m}
\end{bmatrix}^{T}EY\\
EX & = & W^{T}\cdot EY
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
E_{l-1} & = & W_{l}^{T}\cdot E_{l}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where:
\end_layout

\begin_layout Standard
\begin_inset Formula $E_{l-1}$
\end_inset

 represents the error in layer 
\begin_inset Formula $l-1$
\end_inset

 caused by the error 
\begin_inset Formula $E_{l}$
\end_inset

 in layer 
\begin_inset Formula $l$
\end_inset

.
\end_layout

\begin_layout Subsection
Updating Weights
\end_layout

\begin_layout Standard
The aim is to minimize the error function.
\end_layout

\begin_layout Standard
A simple way to do this is to start with random weights, calculate the error
 function for those weights then check the slope 
\begin_inset Formula $\frac{\delta E_{l+1}}{\delta W_{l}}$
\end_inset

 of this function to go downhill.
\end_layout

\begin_layout Standard
The weights can then be updated by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
W_{l} & = & W_{l}-\frac{\delta E_{l+1}}{\delta W_{l}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
Calculate Gradient
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
e_{l} & = & a_{l}-y_{l}\\
\delta w_{l} & = & \frac{1}{m}x_{l}\cdot e_{l}\\
\delta b_{l} & = & \frac{1}{m}\sum_{k=1}^{m}e_{l,k}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Learning Rate
\end_layout

\begin_layout Standard
When there is a strong trend of going in one direction we can take bigger
 steps (larger learning rate) but if the direction keeps changing we should
 take smaller steps (smaller learning rate) to search for a better minimum.
 Learning rate, 
\begin_inset Formula $Lr\in\left[0\cdots1\right]$
\end_inset

, refers to the step size:
\begin_inset Formula 
\begin{eqnarray*}
W_{l} & = & W_{l}-Lr\frac{\delta E_{l+1}}{\delta W_{l}}
\end{eqnarray*}

\end_inset

The learning rate regulates how much the network “learns” in a single iteration.
\end_layout

\begin_layout Subsection
Summary
\end_layout

\begin_layout Standard
Feed forward:
\begin_inset Formula 
\begin{eqnarray*}
Output_{_{l}} & = & Activation\left(Weights_{l-1}\cdot Output_{l-1}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Back Propagate:
\begin_inset Formula 
\begin{eqnarray*}
E_{l-1} & = & W_{l}^{T}\cdot E_{l}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Update weights:
\begin_inset Formula 
\begin{eqnarray*}
W_{l} & = & W_{l}-Lr\frac{\delta E_{l+1}}{\delta W_{l}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Programming Steps
\end_layout

\begin_layout Enumerate
Prepare input data
\end_layout

\begin_layout Enumerate
Initialize weights and intercepts
\end_layout

\begin_layout Enumerate
Forward propagate
\end_layout

\begin_layout Enumerate
Calculate loss
\end_layout

\begin_layout Enumerate
Backward propagate
\end_layout

\begin_layout Enumerate
Update weights and intercepts
\end_layout

\begin_layout Enumerate
Repeat steps 3 through 6 specified times
\end_layout

\begin_layout Enumerate
Predict result.
\end_layout

\begin_layout Section
Definitions
\end_layout

\begin_layout Standard
NL number of neural layers
\end_layout

\begin_layout Standard
LU list of layer sizes for layers; 
\begin_inset Formula $LU\left(1\right)$
\end_inset

 = number of features
\end_layout

\begin_layout Section
Initialization
\end_layout

\begin_layout Standard
\begin_inset Formula $CF$
\end_inset

 list of 
\begin_inset Formula $NL-1$
\end_inset

 matrices
\end_layout

\begin_layout Standard
\begin_inset Formula $I$
\end_inset

 list of 
\begin_inset Formula $NL-1$
\end_inset

 arrays
\end_layout

\begin_layout Standard
\begin_inset Formula $CF_{l}\left[fi,fo\right]$
\end_inset

 matrix of 
\begin_inset Formula $fi$
\end_inset

 rows x 
\begin_inset Formula $fo$
\end_inset

 columns
\end_layout

\begin_layout Standard
\begin_inset Formula $I_{l}\left[fo\right]$
\end_inset

 array of 
\begin_inset Formula $fo$
\end_inset

 rows
\end_layout

\begin_layout Standard
where 
\begin_inset Formula $l$
\end_inset

 is layer number, 
\begin_inset Formula $l\in\left(1,\cdots,NL-1\right)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $fi$
\end_inset

 is number of input neurons for layer 
\begin_inset Formula $l$
\end_inset

; 
\begin_inset Formula $fi=LU\left(l\right)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $fo$
\end_inset

 is number of output neurons for layer 
\begin_inset Formula $l$
\end_inset

; 
\begin_inset Formula $fo=LU\left(l+1\right)$
\end_inset


\end_layout

\begin_layout Standard
Initialize each element of 
\begin_inset Formula $CF$
\end_inset

 and 
\begin_inset Formula $I$
\end_inset

 to a random number in a specified range
\end_layout

\begin_layout Section
Process Batch
\end_layout

\begin_layout Standard
NS number of samples
\end_layout

\begin_layout Standard
NF number of features
\end_layout

\begin_layout Standard
NC number of classes
\end_layout

\begin_layout Standard
X matrix of NS rows x NF columns
\end_layout

\begin_layout Standard
Y matrix of NS x NC
\end_layout

\begin_layout Standard
\begin_inset Formula $A$
\end_inset

 list of Activation matrices
\end_layout

\begin_layout Standard
\begin_inset Formula $A_{1}=X$
\end_inset


\end_layout

\begin_layout Subsection
Forward Pass
\end_layout

\begin_layout Standard
Calculate the activation value for layers 
\begin_inset Formula $2\cdots NL$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $A_{l+1}=A_{l}\cdot CF_{l}$
\end_inset

 + 
\begin_inset Formula $I_{l}$
\end_inset


\end_layout

\begin_layout Subsection
Backward Propagation
\end_layout

\begin_layout Standard
D list of NL - 1 matrices
\end_layout

\begin_layout Standard
\begin_inset Formula $GC$
\end_inset

 list of 
\begin_inset Formula $NL-1$
\end_inset

 matrices
\end_layout

\begin_layout Standard
\begin_inset Formula $GI$
\end_inset

 list of 
\begin_inset Formula $NL-1$
\end_inset

 arrays
\end_layout

\begin_layout Standard
\begin_inset Formula $GC_{l}\left[fi,fo\right]$
\end_inset

 matrix of 
\begin_inset Formula $fi$
\end_inset

 rows x 
\begin_inset Formula $fo$
\end_inset

 columns
\end_layout

\begin_layout Standard
\begin_inset Formula $GI_{l}\left[fo\right]$
\end_inset

 array of 
\begin_inset Formula $fo$
\end_inset

 rows
\end_layout

\begin_layout Standard
\begin_inset Formula $D_{NL-1}=A_{NL}-Y$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $GC_{NL}=\left(A_{l}^{T}\cdot D_{l}+\alpha*CF_{l}\right)/NS$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $GI_{NL}[row]=Mean\left(D_{l}\left[row\right]\right)$
\end_inset


\end_layout

\begin_layout Subsubsection
Compute Loss Gradient
\end_layout

\begin_layout Standard
\begin_inset Formula $NCG=A_{l}^{T}*D_{l}$
\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "1"
key "unn"
literal "false"

\end_inset

https://becominghuman.ai/understanding-neural-networks-2-the-math-of-neural-netwo
rks-in-3-equations-6085fd3f09df
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "2"
key "bsnn"
literal "false"

\end_inset

https://towardsdatascience.com/building-a-simple-neural-network-from-scratch-a5c6
b2eb0c34
\end_layout

\end_body
\end_document

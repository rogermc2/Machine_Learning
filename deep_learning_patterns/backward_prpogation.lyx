#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 15mm
\topmargin 10mm
\rightmargin 10mm
\bottommargin 10mm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Backward Propogation
\end_layout

\begin_layout Standard
Overview of forward propagation equations:
\begin_inset Formula 
\begin{eqnarray*}
x & = & a^{\left(1\right)}\,\,\,\,input\,layer\\
z^{\left(2\right)} & = & W^{\left(1\right)}x+b^{\left(1\right)}\,\,\,\,neuron\,value\,at\,hidden\,layer\,1\\
a^{\left(2\right)} & = & f\left(z^{\left(2\right)}\right)\,\,\,\,activation\,value\,at\,hidden\,layer\,1\\
z^{\left(3\right)} & = & W^{\left(2\right)}a^{\left(2\right)}+b^{\left(2\right)}\,\,\,\,neuron\,value\,at\,hidden\,layer\,2\\
a^{\left(3\right)} & = & f\left(z^{\left(2\right)}\right)\,\,\,\,activation\,value\,at\,hidden\,layer\,2\\
s & = & W^{\left(3\right)}a^{\left(3\right)}\,\,\,\,output\,value\,predicted\,by\,output\,layer
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Cost function (for example mean squared error):
\begin_inset Formula 
\begin{eqnarray*}
C & = & cost\left(s,y\right)\\
 & = & MSE\left(s,y\right)\,\,\,\,for\,example
\end{eqnarray*}

\end_inset

where:
\end_layout

\begin_layout Standard
\begin_inset Formula $s$
\end_inset

 is the value predicted by the output layer,
\end_layout

\begin_layout Standard
\begin_inset Formula $y$
\end_inset

 is the expected value (label) from the input data.
\end_layout

\begin_layout Standard
Based on the value C, the model “knows” how much to adjust its parameters
 to get closer to the expected output y.
 This adjusment is done by the backpropagation algorithm.
\end_layout

\begin_layout Standard
Feeding backward happens by means of the partial derivatives of the feedforward
 functions.
 
\end_layout

\begin_layout Standard
Backpropagation aims to minimize the cost C by adjusting the network’s weights
 and biases.
\end_layout

\begin_layout Standard
The level of adjustment is determined by 
\series bold
the gradients of the cost function with respect to the network’s weights
 and biases
\series default
.
\end_layout

\begin_layout Standard
The derivative of a function measures the sensitivity to a function value's
 change with respect to a change in the cost function argument x (input
 value).
 
\end_layout

\begin_layout Standard
In other words, the derivative tells us which direction the cost function
 is going.
 
\end_layout

\begin_layout Standard
The gradient of a function 
\begin_inset Formula $C(x_{1},x_{2},…,x_{m})$
\end_inset

 at a point 
\series bold
x
\series default
 is a vector of the partial derivatives of C in 
\series bold
x
\series default
.
\end_layout

\begin_layout Standard
The gradient shows how much 
\series bold
x
\series default
 needs to change (in positive or negative direction) to minimize C.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial C}{\partial\mathbf{x}} & = & \left[\frac{\partial C}{\partial x_{1}},\frac{\partial C}{\partial x_{2}},\cdots,\frac{\partial C}{\partial x_{m}}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Equations for the derivative 
\begin_inset Formula $\frac{\partial C}{\partial w_{jk}^{l}}$
\end_inset

 of C for a single weight 
\begin_inset Formula $w_{jk}^{l}$
\end_inset

 of layer 
\begin_inset Formula $l$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial C}{\partial w_{jk}^{l}} & = & \frac{\partial C}{\partial z_{jk}^{l}}\frac{\partial z_{jk}^{l}}{\partial w_{jk}^{l}}\,\,\,\,chain\,rule
\end{eqnarray*}

\end_inset

where:
\end_layout

\begin_layout Standard
\begin_inset Formula $j$
\end_inset

 is the output index,
\end_layout

\begin_layout Standard
\begin_inset Formula $k$
\end_inset

 is the input index.
\begin_inset Formula 
\begin{eqnarray*}
z_{j}^{l} & = & \sum_{k=l}^{m}w_{jk}^{l}a_{k}^{l-1}+b_{j}^{l}\,\,\,\,by\,definition\\
\frac{\partial z_{j}^{l}}{\partial w_{jk}^{l}} & = & a_{k}^{l-1}\,\,\,\,by\,differentiation\\
\frac{\partial C}{\partial w_{jk}^{l}} & = & \frac{\partial C}{\partial z_{j}^{l}}a_{k}^{l-1}\,\,\,\,final\,value
\end{eqnarray*}

\end_inset

where:
\end_layout

\begin_layout Standard
\begin_inset Formula $m$
\end_inset

 is the number of neurons in layer 
\begin_inset Formula $l-1$
\end_inset

.
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial z_{j}^{l}}{\partial w_{jk}^{l}} & = & a_{k}^{l-1}\,\,\,\,by\,differentiation\\
\frac{\partial C}{\partial w_{jk}^{l}} & = & \frac{\partial C}{\partial z_{j}^{l}}a_{k}^{l-1}\,\,\,\,final\,value
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Equations for the derivative 
\begin_inset Formula $\frac{\partial C}{\partial b_{j}^{l}}$
\end_inset

 of C for a single bias value 
\begin_inset Formula $b_{j}^{l}$
\end_inset

 of layer 
\begin_inset Formula $l$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial C}{\partial b_{j}^{l}} & = & \frac{\partial C}{\partial z_{j}^{l}}\frac{\partial z_{j}^{l}}{\partial b_{j}^{l}}\,\,\,\,chain\,rule\\
\frac{\partial z_{j}^{l}}{\partial b_{j}^{l}} & = & 1\,\,\,\,by\,differentiation\\
\frac{\partial C}{\partial b_{j}^{l}} & = & \frac{\partial C}{\partial z_{j}^{l}}\times1\,\,\,\,final\,value
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The common part in both sets of equations is often called the “local gradient”
 
\begin_inset Formula $\delta_{j}^{i}$
\end_inset

 and is expressed as: 
\begin_inset Formula 
\begin{eqnarray*}
\delta_{j}^{i} & = & \frac{\partial C}{\partial z_{j}^{l}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The gradients allow optimization of the model’s parameters:
\end_layout

\begin_layout LyX-Code
While termination condition not met
\begin_inset Formula 
\begin{eqnarray*}
w & := & w-\epsilon\frac{\partial C}{\partial w}\\
b & := & b-\epsilon\frac{\partial C}{\partial b}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where:
\end_layout

\begin_layout Standard
the initial values of 
\begin_inset Formula $w$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 are set randomly,
\end_layout

\begin_layout Standard
\begin_inset Formula $\epsilon$
\end_inset

 is the learning rate.
\end_layout

\begin_layout Standard
Example calculation of the gradient of C with respect to a single weight
 
\begin_inset Formula $w_{22}^{\left(2\right)}$
\end_inset

.
\end_layout

\begin_layout Standard
Weight 
\begin_inset Formula $w_{22}^{2}$
\end_inset

 connects 
\begin_inset Formula $a_{2}^{\left(2\right)}$
\end_inset

 and 
\begin_inset Formula $z_{2}^{\left(2\right)}$
\end_inset

 , so computing the gradient requires applying the chain rule through 
\begin_inset Formula $z_{2}^{\left(3\right)}$
\end_inset

 and 
\begin_inset Formula $a_{2}^{\left(3\right)}$
\end_inset

 : 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial C}{\partial w_{22}^{\left(2\right)}} & = & \frac{\partial C}{\partial z_{2}^{\left(3\right)}}\cdot\frac{\partial z_{2}^{\left(3\right)}}{\partial w_{22}^{\left(2\right)}}\\
 & = & \frac{\partial C}{\partial a_{2}^{\left(3\right)}}\cdot\frac{\partial a_{2}^{\left(3\right)}}{\partial w_{22}^{\left(2\right)}}\\
 & = & \frac{\partial C}{\partial a_{2}^{\left(3\right)}}\cdot f'\left(z_{2}^{\left(3\right)}\right)\cdot a_{2}^{\left(3\right)}
\end{eqnarray*}

\end_inset

 Calculating the final value of the derivative of C in 
\begin_inset Formula $a_{2}^{\left(3\right)}$
\end_inset

 requires knowledge of the cost function C.
 
\end_layout

\begin_layout Standard
As C is dependent on 
\begin_inset Formula $a_{2}^{\left(3\right)}$
\end_inset

 , calculating the derivative should be fairly straightforward.
\end_layout

\end_body
\end_document
